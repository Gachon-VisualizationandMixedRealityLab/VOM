field,title,imagelink,text
3,A Preliminary Work: Mixed Reality-integrated Computer-aided Surgical Navigation System for Paranasal Sinus Surgery using Microsoft HoloLens 2,CGI2021-MR.png,"Paranasal sinus surgery has high demands for minimal invasion and safety. Computer-aided surgical navigation (CSN) applications have been recog-nized as the standard of the surgical practice; the operations of a user can be guided with visually complementary data such as preoperative medical imaging. The introduction of new innovation from mixed reality head mounted display (MR-HMD) technologies is a promising research direction for enhanced usability of paranasal sinus CSN applications. The combined use of MR-HMD with CSN provides a physically unified environment where a user's field of view in in-traoperative sites can be augmented with complementary preoperative data, thereby enhancing their situational awareness. In this study, we present an early phase of the MR introduction for paranasal sinus surgery. We developed an alpha version of a commercial paranasal sinus CSN application using 3D Slicer, a dom-inant open-source clinical software development platform, and then implemented a scene sharing extension module. We refer to it as MR-CSN system. It enables a user wearing MR-HMD networked to equip with their MR-enhanced naviga-tion; their navigation using surgical instruments in the intraoperative sites can be aided with the real-time information from the CSN application. The feasibility of our MR-CSN system was evaluated by experimenting a paranasal sinus surgical simulation with a phantom model."
1,Automatic transfer function design for medical direct volume rendering via clustering analysis,JMIHI-2021.jpg,"Transfer Function (TF) design is a central topic in medical direct volume rendering (DVR). TF design allows for interactive identification of features of interest (FOIs) within a medical image volume and their visual emphasis by assigning appropriate optical parameters (opacity and color) to them. Conventional TF design, however, is not intuitive and usually a ¡®trial-and-error¡¯ process for most users. In this work, we propose an automatic TF design scheme which consists of two-steps. First, we introduce a new clustering-based ray analysis (CRA) to automatically identify FOIs along a viewing ray defined by users. Here, the proposed CRA approach uses regional and contextual information around rays to improve the identification capability. Second, the proposed CRA approach automatically generates a TF to emphasize identified FOIs by adopting a visibility-driven TF parameter optimization algorithm. Our experiments show the effectiveness of the proposed CRA approach by demonstrating its advantages over the existing ray analysis approach relying on local intensity profiles of a ray. We evaluate a number of medical image volume datasets to show the utility of the proposed CRA approach for automatic TF design."
2,Optic Disc and Cup Segmentation Through Fuzzy Broad Learning System for Glaucoma Screening,TII2020-Seg.png,"Glaucoma is an ocular disease that causes permanent blindness if not cured at an early stage. Cup-to-disk ratio (CDR), obtained by dividing the height of optic cup (OC) with the height of optic disk (OD), is a widely adopted metric used for glaucoma screening. Therefore, accurately segmenting OD and OC is crucial for calculating a CDR. Most methods have employed deep learning methods for the segmentation of OD and OC. However, these methods are very time consuming. In this article, we present a new fuzzy broad learning system-based technique for OD and OC segmentation with glaucoma screening. We comprehensively integrated extracting a region of interest from RGB images, data augmentation, extracting red and green channel images, and inputting them to the two separate fuzzy broad learning system-based neural networks for segmenting the OD and OC, respectively, and then calculated CDR. Experiments show that our fuzzy broad learning system-based technique outperforms many state-of-the-art methods."
2,"SPST-CNN: spatial pyramid based searching and tagging of liver's intraoperative live views via CNN for minimal invasive surgery	",JBI2021-Seg.png,"Laparoscopic liver surgery is challenging to perform because of compromised ability of the surgeon to localizesubsurface anatomy due to minimal invasive visibility. While image guidance has the potential to address thisbarrier, intraoperative factors, such as insufflations and variable degrees of organ mobilization from supportingligaments, may generate substantial deformation. The navigation ability in terms of searching and taggingwithin liver views has not been characterized, and current object detection methods do not account for themechanics of how these features could be applied to the liver images. In this research, we have proposed spatialpyramid based searching and tagging of liver¡¯s intraoperative views using convolution neural network (SPST-CNN). By exploiting a hybrid combination of an image pyramid at input and spatial pyramid pooling layer atdeeper stages of SPST-CNN, we reveal the gains of full-image representations for searching and tagging variablescaled liver live views. SPST-CNN provides pinpoint searching and tagging of intraoperative liver views to obtainup-to-date information about the location and shape of the area of interest. Downsampling input using imagepyramid enables SPST-CNN framework to deploy input images with a diversity of resolutions for achieving scale-invariance feature. We have compared the proposed approach to the four recent state-of-the-art approaches andour method achieved better mAP up to 85.9 percent."
2,VoxRec: Hybrid Convolutional Neural Network for Active 3D Object Recognition,ACCESS2020-Seg.png,"Deep Neural Network methods have been used to a variety of challenges in automatic 3D recognition. Although discovered techniques provide many advantages in comparison with conventional methods, they still suffer from different drawbacks, e.g., a large number of pre-processing stages and timeconsuming training. In this paper, an innovative approach has been suggested for recognizing 3D models. It contains encoding 3D point clouds, surface normal, and surface curvature, merge them to provide more effective input data, and train it via a deep convolutional neural network on Shapenetcore dataset. We also proposed a similar method for 3D segmentation using Octree coding method. Finally, comparing the accuracy with some of the state-of-the-art demonstrates the effectiveness of our proposed method."
2,OFF-eNET: An Optimally Fused Fully End-to-End Network for Automatic Dense Volumetric 3D Intracranial Blood Vessels Segmentation,TIP2020-Seg.png,"Intracranial blood vessels segmentation from computed tomography angiography (CTA) volumes is a promising biomarker for diagnosis and therapeutic treatment in cerebrovascular diseases. These segmentation outputs are a fundamental requirement in the development of automated decision support systems for preoperative assessment or intraoperative guidance in neuropathology. The state-of-the-art in medical image segmentation methods are reliant on deep learning architectures based on convolutional neural networks. However, despite their popularity, there is a research gap in the current deep learning architectures optimized to address the technical challenges in blood vessel segmentation. These challenges include: (i) the extraction of concrete brain vessels close to the skull; and (ii) the precise marking of the vessel locations. We propose an Optimally Fused Fully end-to-end Network (OFF-eNET) for automatic segmentation of the volumetric 3D intracranial vascular structures. OFF-eNET comprises of three modules. In the first module, we exploit the up-skip connections to enhance information flow, and dilated convolution for detailed preservation of spatial feature map that are designed for thin blood vessels. In the second module, we employ residual mapping along with inception module for speedy network convergence and richer visual representation. For the third module, we make use of the transferred knowledge in the form of cascaded training strategy to gradually optimize the three segmentation stages (basic, complete, and enhanced) to segment thin vessels located close to the skull. All these modules are designed to be computationally efficient. Our OFF-eNET, evaluated using 70 CTA image volumes, resulted in 90.75 percent performance in the segmentation of intracranial blood vessels and outperformed the state-of-the-art counterparts."
3,A web-based multidisciplinary team meeting visualisation system,IJCARS2020-MDT.png,"Multidisciplinary team meetings (MDTs) are the standard of care for safe, effective patient management in modern hospital-based clinical practice. Medical imaging data are often the central discussion points in many MDTs, and these data are typically visualised, by all participants, on a common large display. We propose a Web-based MDT visualisation system (WMDT-VS) to allow individual participants to view the data on their own personal computing devices with the potential to customise the imaging data, i.e. different view of the data to that of the common display, for their particular clinical perspective. We developed the WMDT-VS by leveraging the state-of-the-art Web technologies to support four MDT visualisation features: (1) 2D and 3D visualisations for multiple imaging modality data; (2) a variety of personal computing devices, e.g. smartphone, tablets, laptops and PCs, to access and navigate medical images individually and share the visualisations; (3) customised participant visualisations; and (4) the addition of extra local image data for visualisation and discussion. We outlined these MDT visualisation features on two simulated MDT settings using different imaging data and usage scenarios. We measured compatibility and performances of various personal, consumer-level, computing devices. Our WMDT-VS provides a more comprehensive visualisation experience for MDT participants."
1,A direct volume rendering visualization approach for serial PET?CT scans that preserves anatomical consistency,IJCARS2020-TemporalVis.png,"Our aim was to develop an interactive 3D direct volume rendering (DVR) visualization solution to interpret and analyze complex, serial multi-modality imaging datasets from positron emission tomography?computed tomography (PET?CT). Our approach uses: (i) a serial transfer function (TF) optimization to automatically depict particular regions of interest (ROIs) over serial datasets with consistent anatomical structures; (ii) integration of a serial segmentation algorithm to interactively identify and track ROIs on PET; and (iii) parallel graphics processing unit (GPU) implementation for interactive visualization. Our DVR visualization more easily identifies changes in ROIs in serial scans in an automated fashion and parallel GPU computation which enables interactive visualization. Our approach provides a rapid 3D visualization of relevant ROIs over multiple scans, and we suggest that it can be used as an adjunct to conventional 2D viewing software from scanner vendors."
1,Feature of Interest based Direct Volume Rendering Using Contextual Saliency-driven Ray Profile Analysis,CGF2018-Vis.png,"Direct volume rendering (DVR) visualization helps interpretation because it allows users to focus attention on the subset of volumetric data that is of most interest to them. The ideal visualization of the features of interest (FOIs) in a volume, however, is still a major challenge. The clear depiction of FOIs depends on accurate identification of the FOIs and appropriate specification of the optical parameters via transfer function (TF) design and it is typically a repetitive trial-and-error process. We address this challenge by introducing a new method that uses contextual saliency information to group the voxels along a viewing ray into distinct FOIs where ¡®contextual saliency¡¯ is a biologically inspired attribute that aids the identification of features that the human visual system considers important. The saliency information is also used to automatically define the optical parameters that emphasize the visual depiction of the FOIs in DVR. We demonstrate the capabilities of our method by its application to a variety of volumetric datasets and highlight its advantages by comparison to current state-of-the-art ray profile analysis methods."
1,Occlusion and Slice-Based Volume Rendering Augmentation for PET-CT,JBHI2017-Vis.png,"Dual-modalitypositron emission tomography and computed tomography (PET-CT) depicts pathophysiological function with PET in an anatomical context provided by CT. Three-dimensional volume rendering approaches enable visualization of a two-dimensional slice of interest (SOI) from PET combined with direct volume rendering (DVR) from CT. However, because DVR depicts the whole volume, it may occlude a region of interest, such as a tumor in the SOI. Volume clipping can eliminate this occlusion by cutting away parts of the volume, but it requires intensive user involvement in deciding on the appropriate depth to clip. Transfer functions that are currently available can make the regions of interest visible, but this often requires complex parameter tuning and coupled preprocessing of the data to define the regions. Hence, we propose a new visualization algorithm where an SOI from PET is augmented by volumetric contextual information from a DVR of the counterpart CT so that the obtrusiveness from the CT in the SOI is minimized. Our approach automatically calculates an augmentation depth parameter by considering the occlusion information derived from the voxels of the CT in front of the PET SOI. The depth parameter is then used to generate an opacity weight function that controls the amount of contextual information visible from the DVR. We outline the improvements with our visualization approach compared to other slice-based and our previous approaches. We present the preliminary clinical evaluation of our visualization in a series of PET-CT studies from patients with nonsmall cell lung cancer"
3,Remote monitoring systems for chronic patients on home hemodialysis: field test of a copresence-enhanced design,JMIR2017-Tele.png,"Patients undertaking long-term and chronic home hemodialysis (HHD) are subject to feelings of isolation and anxiety due to the absence of physical contact with their health care professionals and lack of feedback in regards to their dialysis treatments. Therefore, it is important for these patients to feel the ¡°presence¡± of the health care professionals remotely while on hemodialysis at home for better compliance with the dialysis regime and to feel connected with health care professionals. This study presents an HHD system design for hemodialysis patients with features to enhance patient¡¯s perceived ¡°copresence¡± with their health care professionals. Various mechanisms to enhance this perception were designed and implemented, including digital logbooks, emotion sharing, and feedback tools. The mechanism in our HHD system aims to address the limitations associated with existing self-monitoring tools for HHD patients. A field trial involving 3 nurses and 74 patients was conducted to test the pilot implementation of the copresence design in our HHD system. Mixed method research was conducted to evaluate the system, including surveys, interviews, and analysis of system data. Patients created 2757 entries of dialysis cases during the period of study. Altogether there were 492 entries submitted with ¡°Very Happy¡± as the emotional status, 2167 entries with a ¡°Happy¡± status, 56 entries with a ¡°Neutral¡± status, 18 entries with an ¡°Unhappy¡± status, and 24 entries with a ¡°Very unhappy¡± status. Patients felt assured to share their emotions with health care professionals. Health care professionals were able to prioritize the review of the entries based on the emotional status and also felt assured to see patients¡¯ change in mood. There were 989 entries sent with short notes. Entries with negative emotions had a higher percentage of supplementary notes entered compared to the entries with positive and neutral emotions. The qualitative data further showed that the HHD system was able to improve patients¡¯ feelings of being connected with their health care professionals and thus enhance their self-care on HHD. The health care professionals felt better assured with patients¡¯ status with the use of the system and reported improved productivity and satisfaction with the copresence enhancement mechanism. The survey on the system usability indicated a high level of satisfaction among patients and nurses. The copresence enhancement design complements the conventional use of a digitized HHD logbook and will further benefit the design of future telehealth systems."
1,Efficient visibility-driven medical image visualisation via adaptive binned visibility histogram,CMIG2016-Vis.png,"¡®Visibility¡¯ is a fundamental optical property that represents the observable, by users, proportion of the voxels in a volume during interactive volume rendering. The manipulation of this ¡®visibility¡¯ improves the volume rendering processes; for instance by ensuring the visibility of regions of interest (ROIs) or by guiding the identification of an optimal rendering view-point. The construction of visibility histograms (VHs), which represent the distribution of all the visibility of all voxels in the rendered volume, enables users to explore the volume with real-time feedback about occlusion patterns among spatially related structures during volume rendering manipulations. Volume rendered medical images have been a primary beneficiary of VH given the need to ensure that specific ROIs are visible relative to the surrounding structures, e.g. the visualisation of tumours that may otherwise be occluded by neighbouring structures. VH construction and its subsequent manipulations, however, are computationally expensive due to the histogram binning of the visibilities. This limits the real-time application of VH to medical images that have large intensity ranges and volume dimensions and require a large number of histogram bins. In this study, we introduce an efficient adaptive binned visibility histogram (AB-VH) in which a smaller number of histogram bins are used to represent the visibility distribution of the full VH. We adaptively bin medical images by using a cluster analysis algorithm that groups the voxels according to their intensity similarities into a smaller subset of bins while preserving the distribution of the intensity range of the original images. We increase efficiency by exploiting the parallel computation and multiple render targets (MRT) extension of the modern graphical processing units (GPUs) and this enables efficient computation of the histogram. We show the application of our method to single-modality computed tomography (CT), magnetic resonance (MR) imaging and multi-modality positron emission tomography-CT (PET-CT). In our experiments, the AB-VH markedly improved the computational efficiency for the VH construction and thus improved the subsequent VH-driven volume manipulations. This efficiency was achieved without major degradation in the VH visually and numerical differences between the AB-VH and its full-bin counterpart. We applied several variants of the K-means clustering algorithm with varying Ks (the number of clusters) and found that higher values of K resulted in better performance at a lower computational gain. The AB-VH also had an improved performance when compared to the conventional method of down-sampling of the histogram bins (equal binning) for volume rendering visualisation."
1,Visibility-driven PET-CT visualisation with region of interest (ROI) segmentation,TVCJ2013-Vis.png,"Multi-modality (MM) positron emission tomography-computed tomography (PET-CT) visualises biological and physiological functions (from PET) as region of interests (ROIs) within a higher resolution anatomical reference frame (from CT). The need to efficiently assess and assimilate the information from these co-aligned volumes simultaneously has stimulated new visualisation techniques that combine 3D volume rendering with interactive transfer functions to enable efficient manipulation of these volumes. However, in typical MM volume rendering visualisation, the transfer functions for the volumes are manipulated in isolation with the resulting volumes being fused, thus failing to exploit the spatial correlation that exists between the aligned volumes. Such lack of feedback makes MM transfer function manipulation complex and time consuming. Further, transfer function alone is often insufficient to select the ROIs when they have similar voxel properties to those of non-relevant regions. In this study, we propose a new ROI-based MM visibility-driven transfer function (m 2-vtf) for PET-CT visualisation. We present a novel ¡®visibility¡¯ metric, a fundamental optical property that represents how much of the ROIs are visible to the users, and use it to measure the visibility of the ROIs in PET in relation to how it is affected by transfer function manipulations to its counterpart CT. To overcome the difficulty in ROI selection, we provide an intuitive ROI selection tool based on automated PET segmentation. We further present a MM transfer function automation where the visibility metrics from the PET ROIs are used to automate its CT¡¯s transfer function. Our GPU implementation achieved an interactive visualisation of PET-CT with efficient and intuitive transfer function manipulations."
1,An intuitive Sketch-based Transfer Function Design via Contextual and Regional Labelling,CGI2016-Vis.png,"Transfer function (TF) in direct volume rendering serves to identify and emphasize features of interest (FOIs) and their contextual and regional information for improved visualization. Conventional TF design is not intuitive and usually a 'trial-and-error' process for most users. In an intensity-based one-dimensional (1D) histogram TF, for example, a user needs to repetitively adjust intensity ranges (to identify FOIs) and then assign color and opacity values to the selected range (to emphasize FOIs). In this paper, we propose an intuitive sketch-based interaction technique to design TFs. Our technique enables the user to identify FOIs along the user's viewing ray, with the aid of contextual and regional labels automatically derived from two-dimensional (2D) image slices reconstructed from the ray. For FOI identification, the user makes a sketch on the 2D image slice. Our technique automatically generates an intensity-based 1D TF where the opacity and color values of the intensity range for the FOIs are derived according to their distance from the user's viewpoint and this allows all FOIs along the ray to be visible at once. We show the capabilities of our technique with visualizations on different volumetric data sets, and highlight its advantages when compared to the conventional histogram TF design."
2,A Locally Constrained Random Walk Approach for Airway Segmentation of Low-Contrast Computed Tomography (CT) Image,DICTA2015-Seg.png,"Positron emission tomography (PET) combined with computed tomography (CT) is a routine imaging modality for the diagnosis and interpretation of malignant diseases of the thorax. Accurate airway segmentation is critical for the localization of sites of abnormal metabolism detected with PET-CT. The vast majority of published segmentation algorithms, however, are designed for high-resolution CT and these algorithms do not perform well with the low-contrast CT acquired in PET-CT images. In this study, we present a new fully automated airway segmentation algorithm that is optimised to tolerate the image characteristics inherent in lowcontrast CT images. Our algorithm accurately and robustly segments the airway by introducing: (i) a robust multi-atlas initialisation which incorporates shape priori knowledge for seeds derivation; and (ii) a modified knowledge-based random walk segmentation that uses the derived seeds and manipulates the weights of the edge paths in a locally constrained search space. Our proposed algorithm was evaluated on 20 clinical low-contrast CT from PET-CT patient studies and demonstrated better performance in segmentation results against comparative state-of-the-art algorithms. "
1,Exploration of virtual and augmented reality for visual analytics and 3D volume rendering of functional magnetic resonance imaging (fMRI) data ,BDVA2015-Vis.png,"Statistical analysis of functional magnetic resonance imaging (fMRI), such as independent components analysis, is providing new scientific and clinical insights into the data with capabilities such as characterising traits of schizophrenia. However, with existing approaches to fMRI analysis, there are a number of challenges that prevent it from being fully utilised, including understanding exactly what a ¡®significant activity¡¯ pattern is, which structures are consistent and different between individuals and across the population, and how to deal with imaging artifacts such as noise. Interactive visual analytics has been presented as a step towards solving these challenges by presenting the data to users in a way that illuminates meaning. This includes using circular layouts that represent network connectivity and volume renderings with ¡®in situ¡¯ network diagrams. These visualisations currently rely on traditional 2D ¡®flat¡¯ displays with mouse-and-keyboard input. Due to the constrained screen space and an implied concept of depth, they are limited in presenting a meaningful, uncluttered abstraction of the data without compromising on preserving anatomic context. In this paper, we present our ongoing research on fMRI visualisation and discuss the potential for virtual reality (VR) and augmented reality (AR), coupled with gesture-based inputs to create an immersive environment for visualising fMRI data. We suggest that VR/AR can potentially overcome the identified challenges by allowing for a reduction in visual clutter and by allowing users to navigate the data abstractions in a ¡®natural¡¯ way that lets them keep their focus on the visualisations. We present exploratory research we have performed in creating immersive VR environments for fMRI data."
2,Automated saliency-based lesion segmentation in dermoscopic images,EMBC2015-Seg.png,"The segmentation of skin lesions in dermoscopic images is considered as one of the most important steps in computer-aided diagnosis (CAD) for automated melanoma diagnosis. Existing methods, however, have problems with over-segmentation and do not perform well when the contrast between the lesion and its surrounding skin is low. Hence, in this study, we propose a new automated saliency-based skin lesion segmentation (SSLS) that we designed to exploit the inherent properties of dermoscopic images, which have a focal central region and subtle contrast discrimination with the surrounding regions. The proposed method was evaluated on a public dataset of lesional dermoscopic images and was compared to established methods for lesion segmentation that included adaptive thresholding, Chan-based level set and seeded region growing. Our results show that SSLS outperformed the other methods in regard to accuracy and robustness, in particular, for difficult cases."
3,A Web-Based Medical Multimedia Visualisation Interface for Personal Health Records,CBMS2013-Vis.png,"The healthcare industry has begun to utilise webbased systems and cloud computing infrastructure to develop an increasing array of online personal health record (PHR) systems. Although these systems provide the technical capacity to store and retrieve medical data in various multimedia formats, including images, videos, voice, and text, individual patient use remains limited by the lack of intuitive data representation and visualisation techniques. As such, further research is necessary to better visualise and present these records, in ways that make the complex medical data more intuitive. In this study, we present a web-based PHR visualisation system, called the 3D medical graphical avatar (MGA), which was designed to explore webbased delivery of a wide array of medical data types including multi-dimensional medical images; medical videos; text-based data; and spatial annotations. Mapping information was extracted from each of the data types and was used to embed spatial and textual annotations, such as regions of interest (ROIs) and time-based video annotations. Our MGA itself is built from clinical patient imaging studies, when available. We have taken advantage of the emerging web technologies of HTML5 and WebGL to make our application available to a wider base of users and devices. We analysed the performance of our proof-ofconcept prototype system on mobile and desktop consumer devices. Our initial experiments indicate that our system can render the medical data in a fashion that enables interactive navigation of the MGA. "
2,Model reconstruction of real-world 3D objects: An application with Microsoft HoloLens,ModellingBook2020-Vis.png,"Digital reconstruction of 3D real-world objects has long been a fundamental requirement in computer graphics and vision for virtual reality (VR) and mixed reality (MR) applications. In re-cent years, with the availability of portable and lowcost sensing devices, such as the Kinect Sensor, capable of acquiring RGB-Depth data in real-time, has brought about a profound advancement of the object reconstruction approaches. In this chapter, we present our research on using RGB-Depth sensors embedded in the off-the-shelf MR devices such as the Microsoft HoloLens for object model reconstruction. As MR devices are primarily designed to use its RGB-Depth sensors for environmental mapping (via mesh geometry), it lacks the capability for object reconstruction. We fill this gap by proposing a pipeline for an automated ray-casting based texture mapping approach to the object mesh geometry acquirable from HoloLens. Our preliminary results from real-world object reconstructions, with different sizes and shapes, demonstrate that our approach produces acceptable reconstruction quality with efficient computation."
