field,title,imagelink,text
1,A Transfer Function Optimization Using Visual Saliency For Region of Interest-based Direct Volume Rendering,CGI2023-SaliencyVis.png,"Direct volume rendering (DVR) helps data interpretation by enabling users to interactively focus attention on specific regions in a volume that are of most interest to them. The ideal visualization of these regions of interest (ROIs), however, remains a major challenge. The visual attention given to ROIs depends on the appropriate assignment of optical parameters (opacity and/or color) to ROIs as well as other regions via transfer function (TF), and it is typically a repetitive trial-and-error process from a TF scratch. There have been various automated TF optimization approaches to address the extensive user involvement. They fine-tune initial TF parameters in an iterative manner toward satisfying a pre-defined objective metric. In this work, we propose a new TF optimization approach where we introduce a visual saliency-based objective metric, motivated by the conceptional property of visual saliency as a biologically-inspired measure to aid the identification of regions considered important by the human visual system. Our approach is capable of optimizing opacity and color parameters according to the user-defined target visual saliency of ROIs and producing DVR images that direct visual attention to the ROIs. In addition, we provide an intuitive ROI selection via an image-based user interaction that operates directly on an initial DVR space rather than a complex TF parameter space. We outline our approach by applications to a variety of volumetric datasets and highlight its advantages in comparison to current state-of-the-art TF optimization approaches that use a visibility-based objective metric for opacity parameter optimization."
1,A Transfer Function Design Using A Knowledge Database based on Deep Image and Primitive Intensity Profile Features Retrieval,CGI2023-RetrievalVis.jpg,"Direct volume rendering (DVR) is a technique that emphasizes structures of interest (SOIs) within an image volume visually, while simultaneously depicting adjacent regional information, e.g., the spatial location of a structure concerning its neighbors. In DVR, transfer function (TF) plays a key role by enabling accurate identification of SOIs interactively as well as ensuring appropriate visibility of them. TF generation typically involves non-intuitive trial-and-error optimization of rendering parameters, which is time-consuming and inefficient. Attempts at mitigating this manual process have led to approaches that make use of a knowledge database consisting of pre-designed TFs by domain experts. In these approaches, a user navigates the knowledge database to find the most suitable predesigned TF for their input volume to visualize the SOIs. Although these approaches potentially reduce the workload to generate the TFs, they, however, require manual TF navigation of the knowledge database, as well as the likely fine tuning of the selected TF to suit the input. In this work, we propose a TF design approach where we introduce a new content-based retrieval (CBR) to automatically navigate the knowledge database. Instead of pre-designed TFs, our knowledge database contains image volumes with SOI labels. Given an input image volume, our CBR approach retrieves relevant image volumes (with SOI labels) from the knowledge database; the retrieved labels are then used to generate and optimize TFs of the input. This approach does not need any manual TF navigation and fine tuning. For our CBR approach, we introduce a novel volumetric image feature which includes both a local primitive intensity profile along the SOIs and regional spatial semantics available from the co-planar images to the profile. For the regional spatial semantics, we adopt a convolutional neural network to obtain high-level image feature representations. For the intensity profile, we extend the dynamic time warping technique to address subtle alignment differences between similar profiles (SOIs). Finally, we propose a two-stage CBR scheme to enable the use of these two different feature representations in a complementary manner, thereby improving SOI retrieval performance. We demonstrate the capabilities of our approach with comparison to a conventional CBR approach in visualization, where an intensity profile matching algorithm is used, and also with potential use-cases in medical image volume visualization."
3,Automated Marker-less Patient-to-Preoperative Medical Image Registration approach using RGB-D Images and Facial Landmarks for Potential Use in Computed-Aided Surgical Navigation of the Paranasal Sinus,CGI2023-Autoregistration.png,"Paranasal sinus surgery is an established treatment option for chronic rhinosinusitis. Because this surgery is performed inside the nasal cavity, where critical anatomical structures, such as optic nerves and pituitary glands, exist nearby, surgeons usually rely on computer-aided surgical navigation (CSN) to provide a wide field of view in the surgical site and to allow for precise control of surgical instruments. In the CSNs, it is essential to register the surgical site of the actual patient with the corresponding view from the preoperative computed tomography (CT) images. The traditional registration approaches are performed manually by the user or automatically by attaching fiducial markers on both the patient's surgical site and preoperative CT images for every surgery before use. In this work, we propose an automated approach to register patient-to-preoperative CT image without fiducial markers. The proposed approach detected and extracted facial anatomical landmarks in 2D RGB images through the use of deep learning models. These landmarks were located in 3D facial mesh reconstructed from depth images by using unprojection and ray-marching algorithms. The facial landmark pairs acquired from the patient site and the preoperative CT images are then registered with singular value decomposition and iterative closet point algorithms. We demonstrate the registration capability of our approach using Microsoft HoloLens 2, a mixed reality head-mounted display because it facilitates the acquisition of RGB-depth images and the prototype development of in-situ visualization to illustrate how the CT images are properly registered on the target surgical site. We compared our automated marker-less registration approach to the manual counterpart using a facial phantom with three participants. The results show that our approach produces relatively good registration accuracy, with a marginal target registration error of 4.4 mm when compared to the manual counterpart."
3,A Systematic Review: Virtual-reality-based Techniques for Human Exercises and Health Improvement,,"Virtual Reality (VR) has emerged as a new safe and efficient tool for the rehabilitation of many childhood and adulthood illnesses. VR-based therapies have the potential to improve both motor and functional skills in a wide range of age groups through cortical reorganization and the activation of various neuronal connections. Recently, the potential for using serious VR-based games that combine perceptual learning and dichoptic stimulation has been explored for the rehabilitation of ophthalmological and neurological disorders. In  ophthalmology, several clinical studies have demonstrated the ability to use VR training to enhance stereopsis, contrast sensitivity, and visual acuity. The use of VR technology provides a significant advantage in training each eye individually without requiring occlusion or penalty. In neurological disorders, the majority of patients undergo recurrent episodes (relapses) of neurological impairment, however, in a few cases (60?80%), the illness progresses over time and becomes chronic, consequential in cumulated motor disability and cognitive deficits. Current research on memory restoration has been spurred by theories about brain plasticity and findings concerning the nervous system¡¯s capacity to reconstruct cellular synapses as a result of interaction with enriched environments. Therefore, the use of VR training can play an important role in the improvement of cognitive function and motor disability. Although there are several reviews in the community employing relevant Artificial Intelligence in healthcare, VR has not yet been thoroughly examined in this regard. In this systematic review, we examine the key ideas of VR-based training for prevention and control measurements in ocular diseases such as Myopia, Amblyopia, Presbyopia, and Age-related Macular Degeneration (AMD), and neurological disorders such as Alzheimer, Multiple Sclerosis (MS) Epilepsy and Autism spectrum disorder. This review highlights the fundamentals of VR technologies regarding their clinical research in healthcare. Moreover, these findings will raise community awareness of using VR training and help researchers to learn new techniques to prevent and cure di??erent diseases. We further discuss the current challenges of using VR devices, as well as the future prospects of human training."
2,SThy-Net: A Feature Fusion Enhanced Dense-Branched Modules Network for Small Thyroid Nodule Classification from Ultrasound Image,CGI2023-ThyroidClassification.jpg,"Deep learning studies of thyroid nodule classification from ultrasound (US) images have focused mainly on nodules with diameters > 1 cm. However, small thyroid nodules measuring ¡Â 1 cm, especially nodules with high-risk stratification, are prevalent in the population but without enough focus, including papillary thyroid microcarcinoma (PTMC) as their common malignant type. Additionally, small nodules with high-risk stratification are difficult for physicians to diagnose from US images due to their atypical features. In this work, we propose a small thyroid nodule classification network (SThy-Net) to classify benign and PTMC small thyroid nodules with high-risk stratification from US images. We design two main components, a dense-branched module and a Gaussian-enhanced feature fusion module, to help recognize small thyroid nodules. To our knowledge, this work is the first to address the challenging task of classifying small thyroid nodules using US images. Our SThy-Net achieves as high accuracy as 87.4% compared to five state-of-the-art thyroid nodule diagnosis studies, several state-of-the-art deep learning models, and three radiologists. From visual explainability, our network shows an intuitive feature extraction method and consistency with US image analysis of radiologists. The results suggest that our network has the potential to be an affordable tool for radiologists to diagnose small nodules with high-risk stratification in clinical practice."
2,Abdominal Aortic Thrombus Segmentation in Postoperative Computed Tomography Angiography Images Using Bi-Directional Convolutional Long Short-TermMemory Architecture,Sensors2023-AAASeg.jpg,"Abdominal aortic aneurysm (AAA) is a fatal clinical condition with high mortality. Computed tomography angiography (CTA) imaging is the preferred minimally invasive modality for the long-term postoperative observation of AAA. Accurate segmentation of the thrombus region of interest (ROI) in a postoperative CTA image volume is essential for quantitative assessment and rapid clinical decision making by clinicians. Few investigators have proposed the adoption of convolutional neural networks (CNN). Although these methods demonstrated the potential of CNN architectures by automating the thrombus ROI segmentation, the segmentation performance can be further improved. The existing methods performed the segmentation process independently per 2D image and were incapable of using adjacent images, which could be useful for the robust segmentation of thrombus ROIs. In this work, we propose a thrombus ROI segmentation method to utilize not only the spatial features of a target image, but also the volumetric coherence available from adjacent images. We newly adopted a recurrent neural network, bi-directional convolutional long short-term memory (Bi-CLSTM) architecture, which can learn coherence between a sequence of data. This coherence learning capability can be useful for challenging situations, for example, when the target image exhibits inherent postoperative artifacts and noises, the inclusion of adjacent images would facilitate learning more robust features for thrombus ROI segmentation. We demonstrate the segmentation capability of our Bi-CLSTM-based method with a comparison of the existing 2D-based thrombus ROI segmentation counterpart as well as other established 2D- and 3D-based alternatives. Our comparison is based on a large-scale clinical dataset of 60 patient studies (i.e., 60 CTA image volumes). The results suggest the superior segmentation performance of our Bi?CLSTM-based method by achieving the highest scores of the evaluation metrics, e.g., our Bi-CLSTM results were 0.0331 higher on total overlap and 0.0331 lower on false negative when compared to 2D U-net++ as the second-best."
2,Automatic Detection and Segmentation of Thrombi in Abdominal Aortic Aneurysms Using a Mask Region-Based Convolutional Neural Network with Optimized Loss Functions,Sensors2022-AAASeg.png,"The detection and segmentation of thrombi are essential for monitoring the disease progression of abdominal aortic aneurysms (AAAs) and for patient care and management. As they have inherent capabilities to learn complex features, deep convolutional neural networks (CNNs) have been recently introduced to improve thrombus detection and segmentation. However, investigations into the use of CNN methods is in the early stages and most of the existing methods are heavily concerned with the segmentation of thrombi, which only works after they have been detected. In this work, we propose a fully automated method for the whole process of the detection and segmentation of thrombi, which is based on a well-established mask region-based convolutional neural network (Mask R-CNN) framework that we improve with optimized loss functions. The combined use of complete intersection over union (CIoU) and smooth L1 loss was designed for accurate thrombus detection and then thrombus segmentation was improved with a modified focal loss. We evaluated our method against 60 clinically approved patient studies (i.e., computed tomography angiography (CTA) image volume data) by conducting 4-fold cross-validation. The results of comparisons to multiple other state-of-the-art methods suggested the superior performance of our method, which achieved the highest F1 score for thrombus detection (0.9197) and outperformed most metrics for thrombus segmentation."
2,SparseVoxNet: 3D Object Recognition with Sparsely Aggregation of 3D Dense Blocks,TNNLS2022-PointCloud.png,"Automatic recognition of 3D objects in a 3D model by convolutional neural network (CNN) methods has been successfully applied to various tasks, e.g., robotics and augmented reality. 3D object recognition is mainly performed by analyzing the object using multi-view images, depth images, graphs, or volumetric data. In some cases, using volumetric data provides the most promising results. However, existing recognition techniques on volumetric data have many drawbacks, such as losing object details on converting points to voxels and the large size of the input volume data that leads to substantial 3D CNNs. Using point clouds could also provide very promising results; however, point cloud based methods typically need sparse data entry and time-consuming training stages. Thus using volumetric could be a more efficient and flexible recognizer for our special case in the Medical school of Shanghai Jiaotong University. In this paper, we propose a novel solution to 3D object recognition from volumetric data using a combination of three compact CNN models, low-cost SparseNet, and feature representation technique. We achieve an optimized network by estimating extra geometrical information comprising the surface normal and curvature into two separated neural networks. These two models provide supplementary information to each voxel data that consequently improve the results. The primary network model takes advantage of all the predicted features and uses these features in Random Forest for recognition purposes. Our method outperforms other methods in training speed in our experiments and provides an accurate result as good as the state-of-the-art. "
2,Experimental Protocol Designed to Employ Nd YAG Laser Surgery for Anterior Chamber Glaucoma Detection via UBM,IET2022-NdYaG.png,"Angle closure glaucoma leads to fluid deposition in eye, and intraocular pressure occurs that damage the optic nerve, causes blindness and vision loss. Anterior chamber (AC) evaluation is imperative for determining the risk of angle-closure. Previously, techniques were dependent on either Pentacam?Scheimpflug that interprets poor visual information, anterior segment optical coherence tomography is injurious to intercede opaque optical structures. Therefore, in this paper, an experimental protocol is designed for detailed disease analysis based on IBM SPSS statistics via ultrasound biomicroscopy which is superior in evaluating deep structures; first, the affected parameter for AC is analysed, and afterwards the direction that needs laser surgery is explored. Experiments are conducted on large-scale clinical studies from an affiliated hospital in Shanghai, China. The dataset comprised 600 AC images in five directions of 60 subjects. The mean with standard deviation for anterior open distance is 0.15879mm, 0.15863mm, and anterior chamber angle is 18.749, 18.741 for left and right eye respectively. It is found that anterior chamber angle in the downside of the AC is wider than the upside. However, this decision is partly based on the narrowest part of the angle to widen the depth of the direction and eliminate pupil block."
1,Mixed reality hologram slicer (mxdR-HS): a marker-less tangible user interface for interactive holographic volume visualization,arvix2022-TUI.png,"Mixed reality head-mounted displays (mxdR-HMD) have the potential to visualize volumetric medical imaging data in holograms to provide a true sense of volumetric depth. An effective user interface, however, has yet to be thoroughly studied. Tangible user interfaces (TUIs) enable a tactile interaction with a hologram through an object. The object has physical properties indicating how it might be used with multiple degrees-of-freedom. We propose a TUI using a planar object (PO) for the holographic medical volume visualization and exploration. We refer to it as mxdR hologram slicer (mxdR-HS). Users can slice the hologram to examine particular regions of interest (ROIs) and intermix complementary data and annotations. The mxdR-HS introduces a novel real-time ad-hoc marker-less PO tracking method that works with any PO where corners are visible. The aim of mxdR-HS is to maintain minimum computational latency while preserving practical tracking accuracy to enable seamless TUI integration in the commercial mxdR-HMD, which has limited computational resources. We implemented the mxdR-HS on a commercial Microsoft HoloLens with a built-in depth camera. Our experimental results showed our mxdR-HS had a superior computational latency but marginally lower tracking accuracy than two markerbased tracking methods and resulted in enhanced computational latency and tracking accuracy than 10 markerless tracking methods. Our mxdR-HS, in a medical environment, can be suggested as a visual guide to display complex volumetric medical imaging data."
3,Fused Feature Signatures to Probe Tumour Radiogenomics Relationships,SciRep2022-radiogenomics.png,"Radiogenomics relationships (RRs) aims to identify statistically significant correlations between medical image features and molecular characteristics from analysing tissue samples. Previous radiogenomics studies mainly relied on a single category of image feature extraction techniques (ETs); these are (i) handcrafted ETs that encompass visual imaging characteristics, curated from knowledge of human experts and, (ii) deep ETs that quantify abstract-level imaging characteristics from large data. Prior studies therefore failed to leverage the complementary information that are accessible from fusing the ETs. In this study, we propose a fused feature signature (FFSig): a selection of image features from handcrafted and deep ETs (e.g., transfer learning and fine-tuning of deep learning models). We evaluated the FFSigs ability to better represent RRs compared to individual ET approaches with two public datasets: the first dataset was used to build the FFSig using 89 patients with non-small cell lung cancer (NSCLC) comprising of gene expression data and CT images of the thorax and the upper abdomen for each patient; the second NSCLC dataset comprising of 117 patients with CT images and RNA-Seq data and was used as the validation set. Our results show that our FFSig encoded complementary imaging characteristics of tumours and identified more RRs with a broader range of genes that are related to important biological functions such as tumourigenesis. We suggest that the FFSig has the potential to identify important RRs that may assist cancer diagnosis and treatment in the future."
3,An Integrated Review: Virtual Reality Simulation for Disaster Preparedness Training in Hospitals,JMIR2022-IntegrativeReview.png,"A critical component of disaster preparedness in hospitals is experiential education and training of health care professionals. A live drill is a well-established, effective training approach, but cost restraints and logistic constraints make clinical implementation challenging, and training opportunities with live drills may be severely limited. Virtual reality simulation (VRS) technology may offer a viable training alternative with its inherent features of reproducibility, just-in-time training, and repeatability. This integrated review examines the scientific evidence pertaining to the effectiveness of VRS and its practical usefulness in training health care professionals for in-hospital disaster preparedness. A well-known 4-stage methodology was used for the integrated review process. It consisted of problem identification, a literature search and inclusion criteria determination, 2-stage validation and analysis of searched studies, and presentation of findings. A search of diverse publication repositories was performed. They included Web of Science (WOS), PubMed (PMD), and Embase (EMB). The integrated review process resulted in 12 studies being included. Principle findings identified 3 major capabilities of VRS: (1) to realistically simulate the clinical environment and medical practices related to different disaster scenarios, (2) to develop learning effects on increased confidence and enhanced knowledge acquisition, and (3) to enable cost-effective implementation of training programs. The findings from the integrated review suggested that VRS could be a competitive, cost-effective adjunct to existing training approaches. Although the findings demonstrated the applicability of VRS to different training scenarios, these do not entirely cover all disaster scenarios that could happen in hospitals. This integrated review expects that the recent advances of VR technologies can be 1 of the catalysts to enable the wider adoption of VRS training on challenging clinical scenarios that require sophisticated modeling and environment depiction."
3,A Preliminary Work: Mixed Reality-integrated Computer-aided Surgical Navigation System for Paranasal Sinus Surgery using Microsoft HoloLens 2,CGI2021-MR.png,"Paranasal sinus surgery has high demands for minimal invasion and safety. Computer-aided surgical navigation (CSN) applications have been recog-nized as the standard of the surgical practice; the operations of a user can be guided with visually complementary data such as preoperative medical imaging. The introduction of new innovation from mixed reality head mounted display (MR-HMD) technologies is a promising research direction for enhanced usability of paranasal sinus CSN applications. The combined use of MR-HMD with CSN provides a physically unified environment where a user's field of view in in-traoperative sites can be augmented with complementary preoperative data, thereby enhancing their situational awareness. In this study, we present an early phase of the MR introduction for paranasal sinus surgery. We developed an alpha version of a commercial paranasal sinus CSN application using 3D Slicer, a dom-inant open-source clinical software development platform, and then implemented a scene sharing extension module. We refer to it as MR-CSN system. It enables a user wearing MR-HMD networked to equip with their MR-enhanced naviga-tion; their navigation using surgical instruments in the intraoperative sites can be aided with the real-time information from the CSN application. The feasibility of our MR-CSN system was evaluated by experimenting a paranasal sinus surgical simulation with a phantom model."
1,Automatic transfer function design for medical direct volume rendering via clustering analysis,JMIHI-2021.jpg,"Transfer Function (TF) design is a central topic in medical direct volume rendering (DVR). TF design allows for interactive identification of features of interest (FOIs) within a medical image volume and their visual emphasis by assigning appropriate optical parameters (opacity and color) to them. Conventional TF design, however, is not intuitive and usually a 'trial-and-error' process for most users. In this work, we propose an automatic TF design scheme which consists of two-steps. First, we introduce a new clustering-based ray analysis (CRA) to automatically identify FOIs along a viewing ray defined by users. Here, the proposed CRA approach uses regional and contextual information around rays to improve the identification capability. Second, the proposed CRA approach automatically generates a TF to emphasize identified FOIs by adopting a visibility-driven TF parameter optimization algorithm. Our experiments show the effectiveness of the proposed CRA approach by demonstrating its advantages over the existing ray analysis approach relying on local intensity profiles of a ray. We evaluate a number of medical image volume datasets to show the utility of the proposed CRA approach for automatic TF design."
2,Optic Disc and Cup Segmentation Through Fuzzy Broad Learning System for Glaucoma Screening,TII2020-Seg.png,"Glaucoma is an ocular disease that causes permanent blindness if not cured at an early stage. Cup-to-disk ratio (CDR), obtained by dividing the height of optic cup (OC) with the height of optic disk (OD), is a widely adopted metric used for glaucoma screening. Therefore, accurately segmenting OD and OC is crucial for calculating a CDR. Most methods have employed deep learning methods for the segmentation of OD and OC. However, these methods are very time consuming. In this article, we present a new fuzzy broad learning system-based technique for OD and OC segmentation with glaucoma screening. We comprehensively integrated extracting a region of interest from RGB images, data augmentation, extracting red and green channel images, and inputting them to the two separate fuzzy broad learning system-based neural networks for segmenting the OD and OC, respectively, and then calculated CDR. Experiments show that our fuzzy broad learning system-based technique outperforms many state-of-the-art methods."
2,"""SPST-CNN: spatial pyramid based searching and tagging of liver's intraoperative live views via CNN for minimal invasive surgery	",JBI2021-Seg.png,"Laparoscopic liver surgery is challenging to perform because of compromised ability of the surgeon to localizesubsurface anatomy due to minimal invasive visibility. While image guidance has the potential to address thisbarrier, intraoperative factors, such as insufflations and variable degrees of organ mobilization from supportingligaments, may generate substantial deformation. The navigation ability in terms of searching and tagging within liver views has not been characterized, and current object detection methods do not account for themechanics of how these features could be applied to the liver images. In this research, we have proposed spatialpyramid based searching and tagging of liver's intraoperative views using convolution neural network (SPST-CNN). By exploiting a hybrid combination of an image pyramid at input and spatial pyramid pooling layer atdeeper stages of SPST-CNN, we reveal the gains of full-image representations for searching and tagging variablescaled liver live views. SPST-CNN provides pinpoint searching and tagging of intraoperative liver views to obtainup-to-date information about the location and shape of the area of interest. Downsampling input using imagepyramid enables SPST-CNN framework to deploy input images with a diversity of resolutions for achieving scale-invariance feature. We have compared the proposed approach to the four recent state-of-the-art approaches andour method achieved better mAP up to 85.9 percent."
2,VoxRec: Hybrid Convolutional Neural Network for Active 3D Object Recognition,ACCESS2020-Seg.png,"Deep Neural Network methods have been used to a variety of challenges in automatic 3D recognition. Although discovered techniques provide many advantages in comparison with conventional methods, they still suffer from different drawbacks, e.g., a large number of pre-processing stages and timeconsuming training. In this paper, an innovative approach has been suggested for recognizing 3D models. It contains encoding 3D point clouds, surface normal, and surface curvature, merge them to provide more effective input data, and train it via a deep convolutional neural network on Shapenetcore dataset. We also proposed a similar method for 3D segmentation using Octree coding method. Finally, comparing the accuracy with some of the state-of-the-art demonstrates the effectiveness of our proposed method."
2,OFF-eNET: An Optimally Fused Fully End-to-End Network for Automatic Dense Volumetric 3D Intracranial Blood Vessels Segmentation,TIP2020-Seg.png,"Intracranial blood vessels segmentation from computed tomography angiography (CTA) volumes is a promising biomarker for diagnosis and therapeutic treatment in cerebrovascular diseases. These segmentation outputs are a fundamental requirement in the development of automated decision support systems for preoperative assessment or intraoperative guidance in neuropathology. The state-of-the-art in medical image segmentation methods are reliant on deep learning architectures based on convolutional neural networks. However, despite their popularity, there is a research gap in the current deep learning architectures optimized to address the technical challenges in blood vessel segmentation. These challenges include: (i) the extraction of concrete brain vessels close to the skull; and (ii) the precise marking of the vessel locations. We propose an Optimally Fused Fully end-to-end Network (OFF-eNET) for automatic segmentation of the volumetric 3D intracranial vascular structures. OFF-eNET comprises of three modules. In the first module, we exploit the up-skip connections to enhance information flow, and dilated convolution for detailed preservation of spatial feature map that are designed for thin blood vessels. In the second module, we employ residual mapping along with inception module for speedy network convergence and richer visual representation. For the third module, we make use of the transferred knowledge in the form of cascaded training strategy to gradually optimize the three segmentation stages (basic, complete, and enhanced) to segment thin vessels located close to the skull. All these modules are designed to be computationally efficient. Our OFF-eNET, evaluated using 70 CTA image volumes, resulted in 90.75 percent performance in the segmentation of intracranial blood vessels and outperformed the state-of-the-art counterparts."
3,A web-based multidisciplinary team meeting visualisation system,IJCARS2020-MDT.png,"Multidisciplinary team meetings (MDTs) are the standard of care for safe, effective patient management in modern hospital-based clinical practice. Medical imaging data are often the central discussion points in many MDTs, and these data are typically visualised, by all participants, on a common large display. We propose a Web-based MDT visualisation system (WMDT-VS) to allow individual participants to view the data on their own personal computing devices with the potential to customise the imaging data, i.e. different view of the data to that of the common display, for their particular clinical perspective. We developed the WMDT-VS by leveraging the state-of-the-art Web technologies to support four MDT visualisation features: (1) 2D and 3D visualisations for multiple imaging modality data; (2) a variety of personal computing devices, e.g. smartphone, tablets, laptops and PCs, to access and navigate medical images individually and share the visualisations; (3) customised participant visualisations; and (4) the addition of extra local image data for visualisation and discussion. We outlined these MDT visualisation features on two simulated MDT settings using different imaging data and usage scenarios. We measured compatibility and performances of various personal, consumer-level, computing devices. Our WMDT-VS provides a more comprehensive visualisation experience for MDT participants."
1,A direct volume rendering visualization approach for serial PET-CT scans that preserves anatomical consistency,IJCARS2020-TemporalVis.png,"Our aim was to develop an interactive 3D direct volume rendering (DVR) visualization solution to interpret and analyze complex, serial multi-modality imaging datasets from positron emission tomography computed tomography (PET-CT). Our approach uses: (i) a serial transfer function (TF) optimization to automatically depict particular regions of interest (ROIs) over serial datasets with consistent anatomical structures; (ii) integration of a serial segmentation algorithm to interactively identify and track ROIs on PET; and (iii) parallel graphics processing unit (GPU) implementation for interactive visualization. Our DVR visualization more easily identifies changes in ROIs in serial scans in an automated fashion and parallel GPU computation which enables interactive visualization. Our approach provides a rapid 3D visualization of relevant ROIs over multiple scans, and we suggest that it can be used as an adjunct to conventional 2D viewing software from scanner vendors."
1,Feature of Interest based Direct Volume Rendering Using Contextual Saliency-driven Ray Profile Analysis,CGF2018-Vis.png,"Direct volume rendering (DVR) visualization helps interpretation because it allows users to focus attention on the subset of volumetric data that is of most interest to them. The ideal visualization of the features of interest (FOIs) in a volume, however, is still a major challenge. The clear depiction of FOIs depends on accurate identification of the FOIs and appropriate specification of the optical parameters via transfer function (TF) design and it is typically a repetitive trial-and-error process. We address this challenge by introducing a new method that uses contextual saliency information to group the voxels along a viewing ray into distinct FOIs where 'contextual saliency' is a biologically inspired attribute that aids the identification of features that the human visual system considers important. The saliency information is also used to automatically define the optical parameters that emphasize the visual depiction of the FOIs in DVR. We demonstrate the capabilities of our method by its application to a variety of volumetric datasets and highlight its advantages by comparison to current state-of-the-art ray profile analysis methods."
1,Occlusion and Slice-Based Volume Rendering Augmentation for PET-CT,JBHI2017-Vis.png,"Dual-modalitypositron emission tomography and computed tomography (PET-CT) depicts pathophysiological function with PET in an anatomical context provided by CT. Three-dimensional volume rendering approaches enable visualization of a two-dimensional slice of interest (SOI) from PET combined with direct volume rendering (DVR) from CT. However, because DVR depicts the whole volume, it may occlude a region of interest, such as a tumor in the SOI. Volume clipping can eliminate this occlusion by cutting away parts of the volume, but it requires intensive user involvement in deciding on the appropriate depth to clip. Transfer functions that are currently available can make the regions of interest visible, but this often requires complex parameter tuning and coupled preprocessing of the data to define the regions. Hence, we propose a new visualization algorithm where an SOI from PET is augmented by volumetric contextual information from a DVR of the counterpart CT so that the obtrusiveness from the CT in the SOI is minimized. Our approach automatically calculates an augmentation depth parameter by considering the occlusion information derived from the voxels of the CT in front of the PET SOI. The depth parameter is then used to generate an opacity weight function that controls the amount of contextual information visible from the DVR. We outline the improvements with our visualization approach compared to other slice-based and our previous approaches. We present the preliminary clinical evaluation of our visualization in a series of PET-CT studies from patients with nonsmall cell lung cancer."
3,Remote monitoring systems for chronic patients on home hemodialysis: field test of a copresence-enhanced design,JMIR2017-Tele.png,"Patients undertaking long-term and chronic home hemodialysis (HHD) are subject to feelings of isolation and anxiety due to the absence of physical contact with their health care professionals and lack of feedback in regards to their dialysis treatments. Therefore, it is important for these patients to feel the ""presence"" of the health care professionals remotely while on hemodialysis at home for better compliance with the dialysis regime and to feel connected with health care professionals. This study presents an HHD system design for hemodialysis patients with features to enhance patient's perceived ""copresence"" with their health care professionals. Various mechanisms to enhance this perception were designed and implemented, including digital logbooks, emotion sharing, and feedback tools. The mechanism in our HHD system aims to address the limitations associated with existing self-monitoring tools for HHD patients. A field trial involving 3 nurses and 74 patients was conducted to test the pilot implementation of the copresence design in our HHD system. Mixed method research was conducted to evaluate the system, including surveys, interviews, and analysis of system data. Patients created 2757 entries of dialysis cases during the period of study. Altogether there were 492 entries submitted with ""Very Happy"" as the emotional status, 2167 entries with a ""Happy"" status, 56 entries with a ""Neutral"" status, 18 entries with an ""Unhappy"" status, and 24 entries with a ""Very unhappy"" status. Patients felt assured to share their emotions with health care professionals. Health care professionals were able to prioritize the review of the entries based on the emotional status and also felt assured to see patients' change in mood. There were 989 entries sent with short notes. Entries with negative emotions had a higher percentage of supplementary notes entered compared to the entries with positive and neutral emotions. The qualitative data further showed that the HHD system was able to improve patients' feelings of being connected with their health care professionals and thus enhance their self-care on HHD. The health care professionals felt better assured with patients' status with the use of the system and reported improved productivity and satisfaction with the copresence enhancement mechanism. The survey on the system usability indicated a high level of satisfaction among patients and nurses. The copresence enhancement design complements the conventional use of a digitized HHD logbook and will further benefit the design of future telehealth systems."
1,Efficient visibility-driven medical image visualisation via adaptive binned visibility histogram,CMIG2016-Vis.png," 'Visibility' is a fundamental optical property that represents the observable, by users, proportion of the voxels in a volume during interactive volume rendering. The manipulation of this 'visibility' improves the volume rendering processes; for instance by ensuring the visibility of regions of interest (ROIs) or by guiding the identification of an optimal rendering view-point. The construction of visibility histograms (VHs), which represent the distribution of all the visibility of all voxels in the rendered volume, enables users to explore the volume with real-time feedback about occlusion patterns among spatially related structures during volume rendering manipulations. Volume rendered medical images have been a primary beneficiary of VH given the need to ensure that specific ROIs are visible relative to the surrounding structures, e.g. the visualisation of tumours that may otherwise be occluded by neighbouring structures. VH construction and its subsequent manipulations, however, are computationally expensive due to the histogram binning of the visibilities. This limits the real-time application of VH to medical images that have large intensity ranges and volume dimensions and require a large number of histogram bins. In this study, we introduce an efficient adaptive binned visibility histogram (AB-VH) in which a smaller number of histogram bins are used to represent the visibility distribution of the full VH. We adaptively bin medical images by using a cluster analysis algorithm that groups the voxels according to their intensity similarities into a smaller subset of bins while preserving the distribution of the intensity range of the original images. We increase efficiency by exploiting the parallel computation and multiple render targets (MRT) extension of the modern graphical processing units (GPUs) and this enables efficient computation of the histogram. We show the application of our method to single-modality computed tomography (CT), magnetic resonance (MR) imaging and multi-modality positron emission tomography-CT (PET-CT). In our experiments, the AB-VH markedly improved the computational efficiency for the VH construction and thus improved the subsequent VH-driven volume manipulations. This efficiency was achieved without major degradation in the VH visually and numerical differences between the AB-VH and its full-bin counterpart. We applied several variants of the K-means clustering algorithm with varying Ks (the number of clusters) and found that higher values of K resulted in better performance at a lower computational gain. The AB-VH also had an improved performance when compared to the conventional method of down-sampling of the histogram bins (equal binning) for volume rendering visualisation."
1,Visibility-driven PET-CT visualisation with region of interest (ROI) segmentation,TVCJ2013-Vis.png,"Multi-modality (MM) positron emission tomography-computed tomography (PET-CT) visualises biological and physiological functions (from PET) as region of interests (ROIs) within a higher resolution anatomical reference frame (from CT). The need to efficiently assess and assimilate the information from these co-aligned volumes simultaneously has stimulated new visualisation techniques that combine 3D volume rendering with interactive transfer functions to enable efficient manipulation of these volumes. However, in typical MM volume rendering visualisation, the transfer functions for the volumes are manipulated in isolation with the resulting volumes being fused, thus failing to exploit the spatial correlation that exists between the aligned volumes. Such lack of feedback makes MM transfer function manipulation complex and time consuming. Further, transfer function alone is often insufficient to select the ROIs when they have similar voxel properties to those of non-relevant regions. In this study, we propose a new ROI-based MM visibility-driven transfer function(m2-vtf) for PET-CT visualisation. We present a novel 'visibility' metric, a fundamental optical property that represents how much of the ROIs are visible to the users, and use it to measure the visibility of the ROIs in PET in relation to how it is affected by transfer function manipulations to its counterpart CT. To overcome the difficulty in ROI selection, we provide an intuitive ROI selection tool based on automated PET segmentation. We further present a MM transfer function automation where the visibility metrics from the PET ROIs are used to automate its CT's transfer function. Our GPU implementation achieved an interactive visualisation of PET-CT with efficient and intuitive transfer function manipulations."
1,An intuitive Sketch-based Transfer Function Design via Contextual and Regional Labelling,CGI2016-Vis.png,"Transfer function (TF) in direct volume rendering serves to identify and emphasize features of interest (FOIs) and their contextual and regional information for improved visualization. Conventional TF design is not intuitive and usually a 'trial-and-error' process for most users. In an intensity-based one-dimensional (1D) histogram TF, for example, a user needs to repetitively adjust intensity ranges (to identify FOIs) and then assign color and opacity values to the selected range (to emphasize FOIs). In this paper, we propose an intuitive sketch-based interaction technique to design TFs. Our technique enables the user to identify FOIs along the user's viewing ray, with the aid of contextual and regional labels automatically derived from two-dimensional (2D) image slices reconstructed from the ray. For FOI identification, the user makes a sketch on the 2D image slice. Our technique automatically generates an intensity-based 1D TF where the opacity and color values of the intensity range for the FOIs are derived according to their distance from the user's viewpoint and this allows all FOIs along the ray to be visible at once. We show the capabilities of our technique with visualizations on different volumetric data sets, and highlight its advantages when compared to the conventional histogram TF design."
2,A Locally Constrained Random Walk Approach for Airway Segmentation of Low-Contrast Computed Tomography (CT) Image,DICTA2015-Seg.png,"Positron emission tomography (PET) combined with computed tomography (CT) is a routine imaging modality for the diagnosis and interpretation of malignant diseases of the thorax. Accurate airway segmentation is critical for the localization of sites of abnormal metabolism detected with PET-CT. The vast majority of published segmentation algorithms, however, are designed for high-resolution CT and these algorithms do not perform well with the low-contrast CT acquired in PET-CT images. In this study, we present a new fully automated airway segmentation algorithm that is optimised to tolerate the image characteristics inherent in lowcontrast CT images. Our algorithm accurately and robustly segments the airway by introducing: (i) a robust multi-atlas initialisation which incorporates shape priori knowledge for seeds derivation; and (ii) a modified knowledge-based random walk segmentation that uses the derived seeds and manipulates the weights of the edge paths in a locally constrained search space. Our proposed algorithm was evaluated on 20 clinical low-contrast CT from PET-CT patient studies and demonstrated better performance in segmentation results against comparative state-of-the-art algorithms. "
1,Exploration of virtual and augmented reality for visual analytics and 3D volume rendering of functional magnetic resonance imaging (fMRI) data ,BDVA2015-Vis.png,"Statistical analysis of functional magnetic resonance imaging (fMRI), such as independent components analysis, is providing new scientific and clinical insights into the data with capabilities such as characterising traits of schizophrenia. However, with existing approaches to fMRI analysis, there are a number of challenges that prevent it from being fully utilised, including understanding exactly what a 'significant activity' pattern is, which structures are consistent and different between individuals and across the population, and how to deal with imaging artifacts such as noise. Interactive visual analytics has been presented as a step towards solving these challenges by presenting the data to users in a way that illuminates meaning. This includes using circular layouts that represent network connectivity and volume renderings with 'in situ' network diagrams. These visualisations currently rely on traditional 2D 'flat' displays with mouse-and-keyboard input. Due to the constrained screen space and an implied concept of depth, they are limited in presenting a meaningful, uncluttered abstraction of the data without compromising on preserving anatomic context. In this paper, we present our ongoing research on fMRI visualisation and discuss the potential for virtual reality (VR) and augmented reality (AR), coupled with gesture-based inputs to create an immersive environment for visualising fMRI data. We suggest that VR/AR can potentially overcome the identified challenges by allowing for a reduction in visual clutter and by allowing users to navigate the data abstractions in a 'natural' way that lets them keep their focus on the visualisations. We present exploratory research we have performed in creating immersive VR environments for fMRI data."
2,Automated saliency-based lesion segmentation in dermoscopic images,EMBC2015-Seg.png,"The segmentation of skin lesions in dermoscopic images is considered as one of the most important steps in computer-aided diagnosis (CAD) for automated melanoma diagnosis. Existing methods, however, have problems with over-segmentation and do not perform well when the contrast between the lesion and its surrounding skin is low. Hence, in this study, we propose a new automated saliency-based skin lesion segmentation (SSLS) that we designed to exploit the inherent properties of dermoscopic images, which have a focal central region and subtle contrast discrimination with the surrounding regions. The proposed method was evaluated on a public dataset of lesional dermoscopic images and was compared to established methods for lesion segmentation that included adaptive thresholding, Chan-based level set and seeded region growing. Our results show that SSLS outperformed the other methods in regard to accuracy and robustness, in particular, for difficult cases."
3,A Web-Based Medical Multimedia Visualisation Interface for Personal Health Records,CBMS2013-Vis.png,"The healthcare industry has begun to utilise webbased systems and cloud computing infrastructure to develop an increasing array of online personal health record (PHR) systems. Although these systems provide the technical capacity to store and retrieve medical data in various multimedia formats, including images, videos, voice, and text, individual patient use remains limited by the lack of intuitive data representation and visualisation techniques. As such, further research is necessary to better visualise and present these records, in ways that make the complex medical data more intuitive. In this study, we present a web-based PHR visualisation system, called the 3D medical graphical avatar (MGA), which was designed to explore webbased delivery of a wide array of medical data types including multi-dimensional medical images; medical videos; text-based data; and spatial annotations. Mapping information was extracted from each of the data types and was used to embed spatial and textual annotations, such as regions of interest (ROIs) and time-based video annotations. Our MGA itself is built from clinical patient imaging studies, when available. We have taken advantage of the emerging web technologies of HTML5 and WebGL to make our application available to a wider base of users and devices. We analysed the performance of our proof-ofconcept prototype system on mobile and desktop consumer devices. Our initial experiments indicate that our system can render the medical data in a fashion that enables interactive navigation of the MGA. "
2,Model reconstruction of real-world 3D objects: An application with Microsoft HoloLens,ModellingBook2020-Vis.png,"Digital reconstruction of 3D real-world objects has long been a fundamental requirement in computer graphics and vision for virtual reality (VR) and mixed reality (MR) applications. In re-cent years, with the availability of portable and lowcost sensing devices, such as the Kinect Sensor, capable of acquiring RGB-Depth data in real-time, has brought about a profound advancement of the object reconstruction approaches. In this chapter, we present our research on using RGB-Depth sensors embedded in the off-the-shelf MR devices such as the Microsoft HoloLens for object model reconstruction. As MR devices are primarily designed to use its RGB-Depth sensors for environmental mapping (via mesh geometry), it lacks the capability for object reconstruction. We fill this gap by proposing a pipeline for an automated ray-casting based texture mapping approach to the object mesh geometry acquirable from HoloLens. Our preliminary results from real-world object reconstructions, with different sizes and shapes, demonstrate that our approach produces acceptable reconstruction quality with efficient computation."
